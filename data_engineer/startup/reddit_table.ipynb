{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import praw\n",
    "import datetime\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from user_definition import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def praw_setup(client_id, client_secret, user_agent, password, username):\n",
    "    '''\n",
    "    Instantiate the Python Reddit API Wrapper (PRAW)\n",
    "    object in order to access Reddit data.\n",
    "    \n",
    "    client_id = client_id from your app info on \n",
    "        Reddit's dev website\n",
    "    client_secret = client_secret from from your\n",
    "        app info on Reddit's dev website\n",
    "    user_agent = A string representing whoever is\n",
    "        accessing the data. Per Reddit's API rules,\n",
    "        must include your Reddit username.\n",
    "    password = Your reddit account's password.\n",
    "    username = Your reddit username.\n",
    "    '''\n",
    "    reddit = praw.Reddit(\n",
    "        client_id=client_id,\n",
    "        client_secret=client_secret,\n",
    "        user_agent=user_agent,\n",
    "        password=password,\n",
    "        username=username\n",
    "    )\n",
    "    return reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_post_titles_text_and_features(reddit, post_limit, timeframe='day', one_sub=False, sub=None):\n",
    "    '''\n",
    "    Obtain the top n (post_limit) posts in the requested \n",
    "    timeframe on all 11 subreddits, as well as desired attributes. Can also specify if \n",
    "    just want post info on one subreddit.\n",
    "    \n",
    "    reddit = PRAW instance\n",
    "    post_limit = # of posts you want to get\n",
    "    timeframe = hour, day (the default), week, month, year, \n",
    "        or all (which is all time)\n",
    "    one_sub = If True, gathers data on specific subreddit.\n",
    "    sub = Specific subreddit to get data.\n",
    "    '''\n",
    "    regex_pattern = r'1\\.\\s.*\\n2\\.\\s.*'\n",
    "    regex = re.compile(regex_pattern, re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    subreddits = ['RandomTables','BehindTheTables','d100']\n",
    "    table = []\n",
    "    for subreddit in subreddits:\n",
    "        for submission in reddit.subreddit(subreddit).top(limit=post_limit, time_filter=timeframe):\n",
    "            if regex.search(submission.selftext):\n",
    "                table.append([submission.id,\n",
    "                            submission.title,\n",
    "                            submission.selftext, # get post text\n",
    "                            submission.subreddit.display_name,\n",
    "                            datetime.datetime.utcfromtimestamp(submission.created_utc),\n",
    "                            submission.score,\n",
    "                            submission.num_comments,\n",
    "                            submission.total_awards_received])\n",
    "    return pd.DataFrame(table, columns=['post_id', 'post_title', 'post_text', 'post_subreddit',\n",
    "                                           'creation_datetime', 'score', 'num_comments', 'total_awards_received'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments_and_features(reddit, post_limit, timeframe='day', one_sub=False, sub=None):\n",
    "    \"\"\"\n",
    "    Obtain the top n posts' comments from 11 particular subreddits\n",
    "    in requested timeframe, as well as desired attributes.\n",
    "    \n",
    "    reddit = PRAW instance\n",
    "    post_limit = # of desired posts\n",
    "    timeframe = hour, day (the default), week, month, year, \n",
    "        or all (which is all time)\n",
    "    one_sub = If True, gathers data on specific subreddit.\n",
    "    sub = Specific subreddit to get data.\n",
    "    \"\"\"\n",
    "    if one_sub == True:\n",
    "        table = []\n",
    "        for submission in reddit.subreddit(sub).top(limit=post_limit, time_filter=timeframe):\n",
    "            comments = submission.comments[:-1] # not taking into account the MoreComments object\n",
    "            for comment in comments:\n",
    "                match = re.search(r'\\b\\d+d\\d+\\b', comment.body) # Search for pattern\n",
    "                if match:\n",
    "                    table.append([submission.id,\n",
    "                                  comment.id,\n",
    "                                  comment.body,\n",
    "                                  submission.subreddit.display_name,\n",
    "                                  datetime.datetime.utcfromtimestamp(comment.created_utc),\n",
    "                                  comment.score,\n",
    "                                  match.group()]) # Add matched string to table\n",
    "        return pd.DataFrame(table, columns = ['post_id', 'comment_id', 'comment_text',\n",
    "                                              'subreddit', 'creation_datetime', 'comment_karma',\n",
    "                                              'matched_string'])\n",
    "    else:\n",
    "        subreddits = ['RandomTables','BehindTheTables','d100']\n",
    "        table = []\n",
    "        for subreddit in subreddits:\n",
    "            for submission in reddit.subreddit(subreddit).top(limit=post_limit, time_filter=timeframe):\n",
    "                comments = submission.comments[:-1] # not taking into account the MoreComments object\n",
    "                for comment in comments:\n",
    "                    match = re.search(r'\\b\\d+d\\d+\\b', comment.body) # Search for pattern\n",
    "                    if match:\n",
    "                        table.append([submission.id,\n",
    "                                      comment.id,\n",
    "                                      comment.body,\n",
    "                                      submission.subreddit.display_name,\n",
    "                                      datetime.datetime.utcfromtimestamp(comment.created_utc),\n",
    "                                      comment.score,\n",
    "                                      match.group()]) # Add matched string to table\n",
    "        return pd.DataFrame(table, columns = ['post_id', 'comment_id', 'comment_text',\n",
    "                                              'subreddit', 'creation_datetime', 'comment_karma',\n",
    "                                              'matched_string'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _download_reddit_data():\n",
    "    \"\"\"\n",
    "    Create reddit instance and collect data to write to gcs as two csv's,\n",
    "    one for posts, and the other for comments on those posts.\n",
    "    \"\"\"\n",
    "    reddit = praw_setup(client_id, client_secret, user_agent, password, username)\n",
    "    blob_name_posts = f'{yesterday}/posts.csv' # names for the files\n",
    "    blob_name_comments = f'{yesterday}/comments.csv'\n",
    "    \n",
    "    df_posts = get_post_titles_text_and_features(reddit, post_limit=1000, timeframe='year', one_sub=False, sub=None)\n",
    "    df_comments = get_comments_and_features(reddit, post_limit=1000, timeframe='year', one_sub=False, sub=None)\n",
    "\n",
    "    return df_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the post data\n",
    "df_post_1 = _download_reddit_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display which subreddit are the posts coming from \n",
    "print(df_post_1.groupby('post_subreddit').sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the list of posts\n",
    "print(df_post_1['post_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the post to csv and txt\n",
    "df_post_1.to_csv('./data/table.csv')\n",
    "df_post_1['post_text'].to_csv('./data/table_text_.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the txt file \n",
    "\n",
    "# open the input file\n",
    "with open('./data/reddit_post_text.txt') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# define a regular expression to match differnt patterns\n",
    "pattern_https = re.compile(r'https?://\\S+') #HTTPS links\n",
    "pattern_u = re.compile(r'\\s[uU]/\\w+|\\\\u|[\\(\\)\\[\\]/]u|u/') # u/123abc or [\\u] or (\\u) or /u\n",
    "\n",
    "\n",
    "# remove all the HTTPS links from the text\n",
    "text = re.sub(pattern_https, '', text)\n",
    "text = re.sub(pattern_u, '', text)\n",
    "\n",
    "# open the output file and write the modified text \n",
    "with open('./data/reddit_post_text.txt', 'w') as file:\n",
    "    file.write(text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
